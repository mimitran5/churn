---
title: "hw3_p2_churn"
output: html_document
date: "2024-02-02"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This problem is based on one of [Kaggle's Playground Series of competitions](https://www.kaggle.com/docs/competitions). The Playground Series is a nice way to practice building predictive models by "providing interesting and approachable datasets for our community to practice their machine learning skills". 

You do **NOT** need to download any data from Kaggle. I've created a smaller dataset with some other modifications for use in our HW problem. The datafile, `churn.csv`, is available in the `data` subfolder.

This particular [playground dataset involves data about bank customers](https://www.kaggle.com/competitions/playground-series-s4e1) with the target variable being a binary indicator of whether or not the customer left the bank (`Exited`), or "churned". The playground dataset was constructed using another [Kaggle dataset on bank customer churn prediction](https://www.kaggle.com/datasets/shubhammeshram579/bank-customer-churn-prediction). Follow the preceeding link for information about the variables in this dataset. 

This assignment will focus on building simple classification models for
predicting bank customer churn. You'll be doing your work right in this R Markdown document. Feel free to save it first with a modified filename that includes your name. For example, mine would be **hw3_p2_churn_isken.Rmd**.

You'll likely need a bunch of libraries. I've included a few here but you should add any others that you need. If you don't need some of these, feel free to delete such lines.

```{r warning=FALSE, message=FALSE}
library(dplyr)   # Group by analysis and other SQLish things.
library(ggplot2) # Plotting, of course
library(corrplot) # Correlation plots
library(caret)   # Many aspects of predictive modeling
library(skimr)  # An automated EDA tool 
library(gt) # create table from dataframe
library(rpart) # package for fitting decision trees
library(rpart.plot) # create plot for decision trees
library(randomForest) # fit random forest
library(gt)
```

**MAJOR (10%) HACKER EXTRA** Version control

Create a new R Project for this assignment. Put the project under version control with git. Create a private GitHub repository for this project. Use git and GitHub as you go to do commits periodically and push them to your remote repository. After you have completed the assignment and pushed your last commit to your GitHub repo, add me as a Collaborator (my GitHub username is misken) so that I can see your repo.

I cover use of git and GitHub with R Studio in this module on our course web page:

* [http://www.sba.oakland.edu/faculty/isken/courses/mis5470_f23/git_intro.html](http://www.sba.oakland.edu/faculty/isken/courses/mis5470_f23/git_intro.html)

This Hacker Extra is worth 10% of the total number of points in the assignment.

## Step 1: Read in data

Read the `churn.csv` file from the `data` subfolder into a dataframe named `churn`.

```{r read_churn}
churn <- read.csv("./data/churn.csv")
str(churn)
summary(churn)
skim(churn)
```

Use `str`, `summary`, and `skim` to get a sense of the data. 
The binary target variable is `Exited` where 1 indicates that the customer left the bank (they "churned"). You'll notice that some of the fields are numeric and some are character data. You might also notice that there are fewer variables in our churn dataset than in the original Kaggle versions.

## Step 2: Factor conversions

Some of the variables clearly should be factors. Change all of the variables to factors that you think should be. Include an explanation of why each of these variables should be converted to factors.

```{r factor_conversions}
churn[,c(2,3,8,9,11)] <- lapply(churn[,c(2,3,8,9,11)], factor)
str(churn)
```

> Because `Gender`, `HasCrCard`, `IsAtiveMember` , and `Exited`  represents  categorical variables with two levels ( Male and Female, 1 and 0) converting it to a factor would be appropriate. 

> Converting `Geography` column to a factor is also appropriate since it represents categorical data with three distinct countries. It would be easier to perform analysis. 

## Step 3 - Partition into training and test sets

We will use the [caret](https://topepo.github.io/caret/) package to do the partitioning of our data into training and test dataframes. Just run this chunk to create training and test datasets. This way we'll all be working with the same datasets. Notice that the test set is 20% of the full dataset.

```{r partition}
# Simple partition into train (80%) and test (20%) set 
set.seed(687) # Do NOT change this
trainIndex <- createDataPartition(churn$Exited, p = .8, 
                                  list = FALSE, 
                                  times = 1)

churn_train <- churn[as.vector(trainIndex), ]  
churn_test <- churn[-as.vector(trainIndex), ]

```

Find the number of customers and the percentage of customers for the two `Exited` levels. You'll
see that there are about 20% of the bank customers exited.

```{r target_prop_check_train}
number_exited <- table(churn$Exited)
number_exited
prop.table(number_exited)*100
```


## Step 4: EDA

Do some EDA to try to uncover some relationships that may end up being useful in building a predictive model for `Exited`. You learned things in HW2 which should be useful here. You should **ONLY** use `churn_train` for your EDA. You should explore all of the variables.

### Basic statistics

To see an overview of the distribution of numerical variables, we calculate the summary statistics.

```{r}
summary(churn_train)
```

### Histograms and boxplots

We create histogram for the variables `CreditScore`,`Age`,  `Balance`, `EstimatedSalary` by `Exited` status.

```{r}
# CreditScore by Exited
ggplot(churn_train, aes(x=CreditScore)) +
  geom_histogram(bins=30,fill="lightblue") +
  facet_wrap(~Exited) +
  ggtitle("Histogram of Credit Scores by Exited Status")
```

> The distribution of credit scores for customers who have churned (1) and have not churned (0) are approcimately normal with the peak around 650 - 700. It appears that the counts of customers who churned is lower than that of the non-churned customers. However, there is no major difference in credit score distribution between the two groups.


```{r}
# Age by Exited
ggplot(churn_train, aes(x=Age)) +
  geom_histogram(bins=30,fill="lightblue") +
  facet_wrap(~Exited) +
  ggtitle("Histogram of Age by Exited Status")
```

> The distribution of ages for customers who have not churned (0) shows a higher frequency of younger customers, with a peak in the histogram around the age of 30-40 years. The distribution tapers off as age increases. On contrary, The distribution of ages for customers who have churned (1)
is in the older range - the peak is around  the age of 40-50 years old suggesting that middle-aged customers are more likely to churn compared to younger or older customer.


```{r}
# Balance by Exited
ggplot(churn_train, aes(x=Balance)) +
  geom_histogram(bins=30,fill="lightblue") +
  facet_wrap(~Exited) +
  ggtitle("Histogram of Balance by Exited Status")

```

> It looks like we have two sub-balanced categories in each group.  There's a huge spike at 0 balance for non - exited customers, indicating a significant number of customers with a zero or very low balance. Aside from the spike, the distribition [in much lower frequency comapre to the spike] appears to be roughly normal with a slight peak around the balance of $125,000. For the churned plot, the 0 balanced - spike is not as large as in the non-churned. Beyond the "spike", the distribution is roughly normal with the peak balanced around  $125,000 also.

```{r}
# EstimatedSalary by Exited
ggplot(churn_train, aes(x=EstimatedSalary)) +
  geom_histogram(bins=30,fill="lightblue") +
  facet_wrap(~Exited) +
  ggtitle("Histogram of Estimated Salary by Exited Status")

```

> Distributions of estimated salaries for both groups are similar. Although the frequency of non-churned are much higher than churned customers. 

We use Boxplotnto identify potential outliers

```{r}
ggplot(churn_train,aes(x=CreditScore, y=Exited)) +
  geom_boxplot(fill="lightblue")

ggplot(churn_train,aes(x=Age, y=Exited)) +
  geom_boxplot(fill="lightblue")

ggplot(churn_train,aes(x=Balance, y=Exited)) +
  geom_boxplot(fill="lightblue")

ggplot(churn_train,aes(x=EstimatedSalary, y=Exited)) +
  geom_boxplot(fill="lightblue")

ggplot(churn_train,aes(x=NumOfProducts, y=Exited)) +
  geom_boxplot(fill="lightblue")


```

> There are outliers for `CreditScore` and `Age`



We create barplots to visualize the distribution of categorical variables like `Geography`, `Gender`, `HasCrCard`, `IsActiveMember`

```{r}
ggplot(churn_train) +
  geom_bar(aes(x=Geography), fill="lightblue") +
  facet_wrap(~Exited) +
  labs(title = "Bar Plot of Geography by Exited Status")

```

> Comparing the two groups, non-exited customers have higher in number than churn customer. France has the highest number of non-exited customers compared to Germany and Spain. Germany has the lowest number of non-exited customers among the three regions.

```{r}
ggplot(churn_train) +
  geom_bar(aes(x=Gender), fill="lightblue") +
  facet_wrap(~Exited) +
  labs(title = "Bar Plot of Gender by Exited Status")
```

> For the panel who have not exited, the bar for males is taller than the bar for females, indicating that there are more male customers who have not exited compared to female customers, which demonstrate thr fact that here are more female customers who have exited compared to male customers. in the right panel.

```{r}
ggplot(churn_train) +
  geom_bar(aes(x=HasCrCard), fill="lightblue") +
  facet_wrap(~Exited) +
  labs(title = "Bar Plot of Has Credit Card by Exited Status", x="Has Credit Card (1) or not (0)")
```

> The majority of non-exited customers possess a credit card as it is the same for exited customers. However, the difference is less prominent compared to the non-exited customers.

```{r}
ggplot(churn_train) +
  geom_bar(aes(x=IsActiveMember), fill="lightblue") +
  facet_wrap(~Exited) +
  labs(title = "Bar Plot of Member Status by Exited Status", x="Active Member (1) vs Not Active Member (0)")
```

> There are more active members who have not exited compared to not active members.

```{r}

ggplot(churn_train) +
  geom_bar(aes(x=NumOfProducts), fill="lightblue") +
  facet_wrap(~Exited) +
  labs(title = "Bar Plot of Number of Products by Exited Status", x="Number of Products")
```

> Even though `NumOfProducts` is not a categorical variable, but it is only a few integer values, it is easier to see the "distribution" of number if credit cards's owners with bar plot. It seems most of non-exited customers possess 2 credit cards, and majority of the exited customers possess one credit card. 

### Corrlation Among Predictors

Calculate a correlation matrix to understand the relationships between them. 

```{r}
# correlation matrix
corr_matrix <- cor(churn_train[,c(1,4,6,7,10)])
corr_matrix
# correlation plot
corrplot(corr_matrix, order = "hclust")
```

> We see a moderately negative correlation between `Balaance` and `NumOfProducts`.  Other variable pairs show smaller circles, indicating weaker correlations.

## Step 5 - Building and evaluation of predictive classification models

Now that you know a little more about the data, it's time to start building a few classification models for `Exited`. We will start out using overall prediction accuracy as our metric but we might want to consider other metrics.

**QUESTION** Why might overall prediction accuracy not be the most appropriate metric to consider? What other
metrics might be important and why?

> Since we have imbalanced data (about 20% of the bank customers exited, the other ~80% is not), overall prediction accuracy not be the most appropriate metric to consider. This could result in bias accuracy. For instance, majority baseline classifier which always assigns the non-exited would reach ~80% accuracy since it would correctly predict ~80% non-exited.

> Other metrics we might consider are: confusion matrix and ROC. These methods are appropriate for binary response varibles. Confusion matrix provides tables output to see the number of correct and incorrect predictions for each class (true positives, true negatives, false positives, and false negatives). From there, we can calculate metrics such as accuracy (accuracy $=\dfrac{TP+TN}{TP+TN+FP+FN}$). ROC visualizes the trade off between classes. [this part is straight out of the book]For example, we can achieve high recall (True Positive) at the expense of a high False Positive by selecting a threshold that move us to the top right of the graph. 

### Fit a null model

A very simple model would be to simply predict that `Exited` is equal to 0. On the training data we saw that we'd be ~80% accurate.

Let's create this null model and run a confusion matrix on its "predictions" for both the training and the test data.

```{r tree_null, warning=FALSE, message=FALSE}
# Create a vector of 0's
model_train_null <- rep(0, nrow(churn_train)) # null model's predictions for the train set, predicting the negative class for all test instances
model_test_null <- rep(0, nrow(churn_test)) # null model's predictions for the test set, predicting the negative class for all test instances

cm_train_null <- caret::confusionMatrix(as.factor(model_train_null), churn_train$Exited, positive = "1")
cm_train_null

cm_test_null <- caret::confusionMatrix(as.factor(model_test_null), churn_test$Exited, positive = "1")
cm_test_null
```

**QUESTION** A few questions:

* Are you surprised that the performance of the null model is almost identical on test and train? Why or why not?
* Explain the sensitivity and specificity values. 

> No, I'm not surprised that the performance of the null model is almost identical on test and train because null model is a very simple model. It does not attempt to learn anything from the data, thus there is no difference in how it performs on training vs test set. 

> Sensitivity (True Positive Rate) measures the percentage of true positives that are correctly identified by the model. It is calculated as $TP / (TP + FN)$. Since the null model never predicts positive class, the TP is 0, result in sensitivity is 0 (the worst). This means the model is unable to correctly identify any of the positives.

> Specificity (True Negative Rate) measures the percentage of true negatives hat are correctly identified by the model (Specificity=$TN / (TN + FP)$). Since the null model never predicts positive class, specificity is 1. This means the model correctly identifies all the negatives.

So, as we begin fitting more complicated models, remember that we need to outperform the null model to make it worth it to use more complicated models.

Now I'm going to ask you to fit three models:

* a logistic regression model
* a simple decision tree
* a random forest

We covered all three of these modeling techniques in the class notes.

For each model type, you should:

* fit the model on the training data,
* assess the model's performance on the training data using the `confusionMatrix` function,
* use the model to make predictions on the test data,
* assess the model's performance on the test data using the `confusionMatrix` function,
* discuss the results

In your discussion of the results you should talk about things like:

* how accurate is the model in predicting on the test data
* is there evidence of overfitting?
* how does the model do in terms of other metrics like sensitivity and specificity
* other things you deem important.

### Fit logistic regression models

You'll start by creating a logistic regression model to predict `Exited`. Since there are not that many variables, let's use all of them. Here's a code skeleton to help you get started:

**Hint**: There's an easy way to specify your model formula to include all of the predictor variables
without typing out all the variable names. 

```{r lr1_train}
# Fit model to training data
model_lr1 <- glm(Exited ~ .,
                 data=churn_train,
                 family=binomial(link="logit"))


# Make predictions on train set
predict_train_lr1 <- predict(model_lr1, type="response")

## Convert fitted model values to fitted classes. Use 0.5 as the
#  threshold for classifying a case as a 1.

class_train_lr1 <- as.factor(ifelse(predict_train_lr1>0.5,1,0))
                         
cm_train_lr1 <- confusionMatrix(class_train_lr1, churn_train$Exited, positive="1")
cm_train_lr1
```

Now, let's predict on test data.

```{r lr1_test}
# Make prediction on test set
pred_lr1 <- predict(model_lr1, newdata =churn_test, type = "response")

# Convert predicted probabilities to classes using 0.5 as the threshold
class_test_lr1 <- as.factor(ifelse(pred_lr1 > 0.5, 1, 0))
 
# Confusion matrix for test set                         
cm_test_lr1 <- confusionMatrix(class_test_lr1, churn_test$Exited, positive="1")
cm_test_lr1

```

**QUESTION** How did accuracy, sensitivity and specificity change when predicting on test data instead of the training data?

> The accuracy on the test set is very close to the train set ( 82% vs 83% respectively), indicating the model predict very well on the test data. Thus there is no potential of overfitting here.

> Sensitivity (True Positive rate): the test data has the sensitivity of 37.58%, meaning it accurately identifies 37.58% of the actual numbers of customer who exited. It is slightly lower than sensitivity on the training data (38.38%), but the difference is little. 

> Specificity (True Negative rate): the test data has the specificity of 95.27%, meaning it accurately identifies 95.27% of the actual numbers of customer who are not exited. It is slightly lower than specificity on the training data (95.61%).

> Overall, the results on test data are consistent with the model's performance in the train data. However, the sensitivity is still low, suggesting there's a large percentage of customers who are actually exiting is missing. 

Now change the threshold from 0.5 to 0.4 and create a new model using this new threshold. How does the sensitivity and specificity change as compared to our first logistic regression model? Explain why this happens?

```{r increase_sensitivity}
## Convert fitted model values to fitted classes. Use 0.4 as the
#  threshold for classifying a case as a 1.

class_train_lr1_0.4 <- as.factor(ifelse(predict_train_lr1>0.4,1,0))
                         
cm_train_lr1_0.4 <- confusionMatrix(class_train_lr1_0.4, churn_train$Exited, positive="1")
cm_train_lr1_0.4

# Now test data

# Convert predicted probabilities to classes using 0.4 as the threshold
class_test_lr1_0.4 <- as.factor(ifelse(pred_lr1 > 0.4, 1, 0))
 
# Confusion matrix for test set                         
cm_test_lr1_0.4 <- confusionMatrix(class_test_lr1_0.4, churn_test$Exited, positive="1")
cm_test_lr1_0.4


```

> The model with the new threshold of 0.4 gives out higher sensitivity (higher rate of accuately identifies True Positive, i.e. customers who exited) and lower specificity (lower rate of accuately identifies True Negative, i.e customers who did not exit). 

> Changing from 0.5 to 0.4 meaning we are shifting decision boundary, mathematically speaking. We let $P(X=\text{customers who exited})=0.4$. Plug this into the logit function: $log(P(X))=\dfrac{0.4}{1-0.4}=log(2/3) \leftarrow$ log of odds and it is also a new cutoff point on the logit range. Now all the values that are larger than $log(2/3)$ is identified as positive and values that are below $log(2/3)$ is identified as negatives. Thus changing the threshold, we adjusted the probability scale. In our case, decreasing the threshold resulting increasing the sensitivity and lowering speciticity.


### Fit simple decision tree model

Now create a simple decision tree model to predict `Exited`. Again,
use all the variables.

```{r tree1_train}
# Decision tree model
model_tree1 <- rpart(Exited ~ ., data=churn_train)

# Make prediction on train set
class_train_tree1 <- predict(model_tree1, type="class")

# confucsion matrix on train
cm_train_tree1 <- confusionMatrix(class_train_tree1, churn_train$Exited, positive="1")
cm_train_tree1
```

Create a plot of your decision tree.

```{r decision_tree_plot}
rpart.plot(model_tree1)
```

Explain the bottom left node of your tree. What conditions have to be true for a case to end up being classified by that node? What do those three numbers in the node mean? What does the color of the node mean?

> The bottom left note explain: customers whose ages are less than 43 years old and own 2 credit cards or less, is predicted not to exit (class O). And the percentage of customers who fall into this category is 43%  of the sample. The 0.04 represents the proportion of samples in the node that are classified as the positive class (Exited = 1). Since $0.04 < 0.5$, it's identified as class 0.

> In general, each node consists of the following values: the top values, 0 or 1, is the predicted class for that node. The middle value is the the proportion of samples in that node that belong to the positive class (Exited = 1). If the proportion is less than 0.5 then the data point is considered to be in class 0. The bottom value is the percenatage of sample from the data that fall into that node. All nodes before each split must sum to 100%.

Now, let's predict on test data.

```{r tree1_test}

pred_tree1 <- predict(model_tree1, newdata = churn_test, type = "class")

cm_test_tree1 <- confusionMatrix(pred_tree1, churn_test$Exited, positive="1")
cm_test_tree1

```

> The accuracy on both model's performance and the test data are very close. Thus, there is no sign of potential of overfitting here. 

**QUESTION** How does the performance of the decision tree compare to your logistic regression model? 

> For accuracy: logistic regression is 83.09%, decision tree is 85.26%. The decision tree model has a higher accuracy than the logistic regression model. For sensitivity (True Positive %): logistic Regressionis 49.82% and decision tree is 43.66%. The logistic regression model has a higher sensitivity than the decision tree model, meaning it is better at identifying the positive class (customers who exited) than decision tree. For Specificity (True Negative %): logistic regression is 91.89% and decision tree is 96.26%. The decision tree model has a higher specificity than the logistic regression model, meaning it is better at identifying the negative class (customers who did not exit).

## Fit random forest model

Finally, fit a random forest model.

```{r rf1_train}
rf1_train <- randomForest(Exited ~ ., data = churn_train,ntree = 100, mtry = 3, importance = TRUE)
print(rf1_train)

# create confusion matrix for train set
pred_rf1_train <- predict(rf1_train, type = "class")

#confusion matrix on train
confusionMatrix(pred_rf1_train, churn_train$Exited,
                positive = "1")

```

Now, let's predict on test data.

```{r rf1_test}
# make prediction 
pred_rf1 <- predict(rf1_train, newdata = churn_test)

# I ran into an error: `data` and `reference` should be #factors with the same levels.
# First, making sure the actual values are factors with levels "0" and "1"
churn_test$Exited <- factor(churn_test$Exited, levels = c("0", "1"))

#convert the predicted values to a factor with the same levels as the actual values
pred_rf1 <- factor(pred_rf1, levels = c("0", "1"))

# levels of both factors are matching?
levels(churn_test$Exited)
levels(pred_rf1)

# creating the confusion matrix 
confusionMatrix(pred_rf1, churn_test$Exited, positive = "1")

```


**QUESTION** Summarize the performance of all three of your models (logistic, tree, random forest)? Is their evidence of overfitting in any of these model and what is your evidence for your answer? Add code chunks as needed.

> Summarize the performance of all three of your models:

> Logistic Regression: accuracy: 83.09%, sensitivity : 49.82%, specificity: 91.89%

> Decision Tree: accuracy: 85.26%, sensitivity: 43.66%, specificity: 96.26%

> Random Forest: accuracy: 85.76%, sensitivity: 51.77%, specificity: 94.75%

> It appears that the random forest model has the highest accuracy and sensitivity. Decision tree model is the next best one, with accuracy value is very closed to random forest. The logistic regression model has the lowest accuracy and sensitivity among the three.

> Since the values of accuracy, sensitivity, and speciticity between the train model and the test data of all three classification models are similar (values from test data is slightly less than the values in train model, but it is expected), we don't see any potential overfitting here.

**QUESTION** If you had to pick one to use in an actual financial environment, which model would you use and why? As a manager in charge of retention, what model performance metrics are you most interested in? What are the basic tradeoffs you see in terms of the initiatives you might undertake in response to such a model? For example, if you were really interested in reducing the number of customers exiting, maybe there are some things you might do to incent high risk (of exiting) customers to stay. Discuss.

> If I had to pick one model to use in an actual financial environment, I would use the Random Forest model. The Random Forest model has the highest accuracy (85.76%) and highest sensitivity (51.77%) among the three models which is important as it provides information that might help the finacial company to improve the exited rate. 

> A a manager in charge of retention, the model performance metrics that I am most interested in are sensitivity because it measures the model's ability to correctly identify customers who will exit. Thus the company can come up with appropriate initiatives that help reduce the number of customers churning.

> The basic trade-off in terms of initiatives that might be undertaken in response to such a model is cost vs profit. Interventions to retain customers include offerings/swags, discounts, bonus/reward programs, ect, come at a cost. Balancing between the cost of providing incentives and still be profitable is crucial for any company.

**HACKER EXTRA**

Create a variable importance plot for your random forest to try to get a sense of which variables are most important in predicting customers likely to churn. Build another random forest using only the top 5 or so variables suggested by the importance plot. How does the performance of this reduced model compare to the original model?

```{r importance}
rf_important <- varImp(rf1_train)

plot(rf_important)

# # get the top 5 most important var
rf_important_sorted <- rf_important %>%
  arrange(desc(`0`)) 

# View the sorted data frame
print(rf_important_sorted)

```

> The top 5 most important variables are: `NumOfProducts`, `Age`, `IsActiveMember`, `Balance`, and `Geography`.

Build another random forest using only the top 5 or so variables suggested by the importance plot.

```{r}
# create top 5 var train set
churn_train_top5 <- churn_train[,c(2,4,6,7,9,11)]  
churn_test_top5 <- churn_test[,c(2,4,6,7,9,11)]

# random forest model on train set
rf_top5 <- randomForest(Exited~.,data = churn_train_top5, ntree=100)

# make prediction on train set
pred_rf_top5_train <- predict(rf_top5, type = "class")

# confusion matrix on train set
confusionMatrix(pred_rf_top5_train, churn_train_top5$Exited, positive = "1")
```


Now predicting on test data

```{r}
# Predict on the test set
pred_rf_top5_test <- predict(rf_top5, newdata = churn_test_top5)

# confusion matrix on test set
confusionMatrix(pred_rf_top5_test, churn_test_top5$Exited, positive = "1")
```

> Compare to the previous random forest model, the reduced model has better accuracy and specificity but but the sensitivity is slightly lower than the full model, indicating that the model is [still] less effective at identifying positive cases.